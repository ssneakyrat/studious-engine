# Configuration for FFT-Light Singing Voice Synthesis Proof-of-Concept

data:
  hdf5_path: "datasets/gin/binary/gin.h5" # Path to the HDF5 file created by preprocess.py
  train_val_split: 0.95                  # Percentage of data for training
  allow_different_workers: True          # Important for DataLoader with multiple workers

preprocessing:
  # These should match the parameters used in preprocess.py
  sample_rate: 22050
  n_fft: 1024
  hop_length: 256
  n_mels: 80                            # Number of Mel filterbanks - Affects output detail
  fmin: 0
  fmax: 8000

  # Phoneme Handling
  # Define your phoneme map or generate it. Ensure PAD=0.
  # Example: {'PAD': 0, 'SIL': 1, 'AH': 2, 'IY': 3, ...}
  phoneme_map_path: null                # Set path to a .txt/.json map or leave null to auto-generate
  pad_phoneme_id: 0                     # ID used for padding phoneme sequences
  unk_phoneme_id: 1                     # ID used for unknown phonemes encountered
  sil_phonemes: ["SIL", "SP", "SPN"]    # List of silence phonemes for potential processing

model:
  # Core dimensions - Smaller values -> smaller model, faster train/infer, potentially lower quality
  hidden_dim: 192                       # Main dimension for internal representations. Smaller = less capacity. (Try 128, 192, 256)
  encoder_embed_dim: 192                # Dimension for phoneme embeddings.
  midi_embed_dim: 64                    # Dimension for MIDI note embeddings. Smaller is likely fine for MIDI.
  vocab_size: null                      # Will be automatically set based on phoneme map size
  midi_vocab_size: 128                  # Standard MIDI range 0-127
  n_mels: ${preprocessing.n_mels}      # Must match preprocessing output

  # FFT Block Config (Encoder & Decoder) - Fewer layers/heads/dims -> smaller/faster, less expressive power
  fft_n_layers: 3                       # Number of FFT blocks in Encoder & Decoder. Fewer layers = less complex mapping learned. (Try 2-4)
  fft_n_heads: 2                        # Number of attention heads. Fewer heads = less diverse attention patterns. (Keep low, e.g., 2)
  fft_conv_kernel_size: 9               # Kernel size for Conv1D in FFN (often 9, 1). Affects local context modeling.
  fft_conv_ffn_dim: 768                 # Inner dimension of FFN layers (Conv1D). Larger = more capacity within block. (Try hidden_dim*2 to *4)
  dropout: 0.1                          # Dropout rate - Regularization helps prevent overfitting.

training:
  batch_size: 16                        # Adjust based on VRAM (12GB) and sequence length. Smaller if OOM.
  max_epochs: 200                       # Number of training epochs. Increase if loss is still decreasing.
  learning_rate: 0.001                  # Initial learning rate. May need tuning.
  weight_decay: 0.000001                # Adam weight decay (L2 regularization)
  grad_clip_val: 1.0                    # Gradient clipping to prevent exploding gradients.
  num_workers: 4                        # Number of CPU workers for data loading. Increase if GPU is waiting for data. Set to 0 for debugging.
  # Learning Rate Schedule (Noam-style warmup + decay - common for Transformers)
  lr_warmup_steps: 4000                 # Steps for linear learning rate warmup. Helps stabilize early training.
  # lr_anneal_steps: 600000             # Optional: Steps over which to anneal LR after warmup (e.g., cosine decay)
  # lr_anneal_factor: 0.1               # Optional: Factor to reduce LR by at end of annealing

validation:
  num_val_samples_log: 4                # Number of validation samples to plot Mel specs for in TensorBoard
  log_every_n_steps: 100                 # How often to log training loss
  val_check_interval: 1.0               # Run validation every epoch (1.0 = epoch end, 0.25 = 4 times per epoch)

# Seed for reproducibility
seed: 42