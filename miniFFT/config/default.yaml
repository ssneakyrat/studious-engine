# Configuration for FFT-Light Singing Voice Synthesis

data:
  hdf5_path: "datasets/gin/binary/gin.h5" # Path generated by preprocess.py
  # Ensure these match the parameters used in preprocess.py
  n_mels: 80
  sample_rate: 22050
  hop_length: 256
  # Phoneme & MIDI details
  phoneme_vocab_size: 50  # This will be updated dynamically based on actual vocabulary
  midi_range: 88        # Number of unique MIDI notes + padding token
  pad_token_id: 0       # Assuming 0 is used for padding phonemes/midi
  f0_pad_value: 0.0     # Padding value for F0
  mel_pad_value: -11.5  # Approx log-mel silence value, adjust if needed
  # Added for phone map
  use_phone_map: true   # Whether to use dynamically generated phone map

model:
  # Core dimensions
  hidden_dim: 192         # Main dimension for FFT blocks. Smaller reduces VRAM heavily. Affects overall capacity.
  phoneme_emb_dim: 192    # Embedding dim for phonemes. Match hidden_dim for simplicity?
  midi_emb_dim: 64        # Embedding dim for MIDI. Pitch might not need as large dim as phonemes.
  # FFT Block configuration (Encoder & Decoder)
  encoder_layers: 3       # Number of FFT blocks in Encoder. More layers -> more complex patterns, more VRAM/time.
  decoder_layers: 3       # Number of FFT blocks in Decoder.
  encoder_heads: 2        # Attention heads. More heads -> capture diverse relations, slight compute increase.
  decoder_heads: 2
  ffn_conv_kernel_size: 9 # Kernel size in the 1D Conv FFN. Larger -> wider context.
  ffn_hidden_dim: 768     # Inner dimension of FFN (Expansion factor). Larger -> more capacity, more params/VRAM. (e.g., hidden_dim * 4)
  dropout: 0.1            # Dropout rate. Higher -> more regularization, may hurt if too high on small data.

  # F0 Integration (Simple concatenation assumed for now)
  # Consider adding f0_emb_dim if you decide to embed F0 later

training:
  batch_size: 16          # Adjust based on VRAM. Smaller batch -> noisier gradients, slower convergence per epoch. MUST fit in 12GB VRAM with max_seq_len.
  learning_rate: 0.0001   # Crucial. May need tuning. Start low for Transformers.
  adam_beta1: 0.9
  adam_beta2: 0.98
  adam_eps: 1e-9
  weight_decay: 0.01      # AdamW weight decay for regularization
  lr_warmup_steps: 4000   # Steps for linear learning rate warmup. Helps stabilize early training.
  max_epochs: 10         # Total training epochs. Adjust based on convergence.
  num_workers: 4          # DataLoader workers. Depends on CPU cores. Set to 0 for debugging.
  max_seq_len_frames: 300 # CRITICAL VRAM CONTROL. Max Mel frames (~3.5 sec @ 86fps). Longer sequences EXHAUST 12GB VRAM quickly.
  grad_clip_val: 1.0      # Gradient clipping to prevent explosions.
  val_check_interval: 1.0 # Validate every epoch (float) or N steps (int).
  log_every_n_steps: 50   # How often to log loss to TensorBoard.
  num_val_samples_to_log: 4 # Number of validation samples to visualize.

validation: # Parameters specific to validate_module.py
  num_random_samples_per_test: 2 # How many random samples for module validation tests